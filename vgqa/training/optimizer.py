import torch
from typing import Any, Dict, List
from .scheduler import WarmupMultiStepLR, WarmupReduceLROnPlateau, WarmupPolyLR


def update_ema(model: torch.nn.Module, model_ema: torch.nn.Module, decay: float) -> None:
    """Apply exponential moving average update to model weights."""
    with torch.no_grad():
        if hasattr(model, "module"):
            # unwrapping DDP
            model = model.module
        msd = model.state_dict()
        for k, ema_v in model_ema.state_dict().items():
            model_v = msd[k].detach()
            ema_v.copy_(ema_v * decay + (1.0 - decay) * model_v)


def make_optimizer(cfg, model: torch.nn.Module, logger=None):
    """Create optimizer with different learning rates for different modules"""
    # Group parameters by module type
    vis_enc_param = [p for n, p in model.named_parameters() \
                            if (("vis_encoder" in n) and p.requires_grad)]
    text_enc_param = [p for n, p in model.named_parameters() \
                            if (("text_encoder" in n) and p.requires_grad)]
    temp_dec_param = [p for n, p in model.named_parameters() \
                            if (("ground_decoder.time_decoder" in n) and p.requires_grad)]
    verb_class_param = [p for n, p in model.named_parameters() \
                            if (("_clas" in n) and p.requires_grad)]
    rest_param = [p for n, p in model.named_parameters() if (('vis_encoder' not in n) and ('_clas' not in n) and \
                ('text_encoder' not in n) and ("ground_decoder.time_decoder" not in n) and p.requires_grad)]

    # Get optimizer hyperparameters
    base_lr = cfg.SOLVER.BASE_LR
    optim_type = cfg.SOLVER.OPTIMIZER
    weight_decay = cfg.SOLVER.WEIGHT_DECAY

    # Set different learning rates for different modules
    param_list = [
        {"params" : rest_param},
        {"params" : vis_enc_param, "lr" : cfg.SOLVER.VIS_BACKBONE_LR},
        {"params" : text_enc_param, "lr" : cfg.SOLVER.TEXT_LR},
        {"params" : temp_dec_param, "lr" : cfg.SOLVER.TEMP_LR},
        {"params": verb_class_param, "lr": cfg.SOLVER.VERB_LR},
    ]

    # Create optimizer based on specified type
    if optim_type == 'rmsprop':
        optimizer = torch.optim.RMSprop(param_list, lr=base_lr, weight_decay=weight_decay)
    elif optim_type == 'adamw':
        optimizer = torch.optim.AdamW(param_list, lr=base_lr, weight_decay=weight_decay)
    elif optim_type == 'adam':
        optimizer = torch.optim.Adam(param_list, lr=base_lr, weight_decay=weight_decay)
    elif optim_type== 'sgd':
        optimizer = torch.optim.SGD(param_list, lr=base_lr, weight_decay=weight_decay, momentum=cfg.SOLVER.MOMENTUM)
    else:
        raise ValueError(f"Unsupported optimizer type: {optim_type}")

    return optimizer


def make_lr_scheduler(cfg, optimizer: torch.optim.Optimizer, logger=None):
    """Create learning rate scheduler based on configuration"""
    if cfg.SOLVER.SCHEDULE.TYPE == "WarmupMultiStepLR":
        return WarmupMultiStepLR(
            optimizer,
            cfg.SOLVER.STEPS,
            cfg.SOLVER.GAMMA,
            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.WARMUP_METHOD,
        )
    
    elif cfg.SOLVER.SCHEDULE.TYPE == "WarmupReduceLROnPlateau":
        return WarmupReduceLROnPlateau(
            optimizer,
            cfg.SOLVER.SCHEDULE.FACTOR,
            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.WARMUP_METHOD,
            patience=cfg.SOLVER.SCHEDULE.PATIENCE,
            threshold=cfg.SOLVER.SCHEDULE.THRESHOLD,
            cooldown=cfg.SOLVER.SCHEDULE.COOLDOWN,
            logger=logger,
        )
    elif cfg.SOLVER.SCHEDULE.TYPE == "WarmupPolyLR":
        return WarmupPolyLR(
            optimizer,
            cfg.SOLVER.POWER,
            cfg.SOLVER.MAX_ITER,
            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,
            warmup_iters=cfg.SOLVER.WARMUP_ITERS,
            warmup_method=cfg.SOLVER.WARMUP_METHOD,
        )
    else:
        raise ValueError(f"Invalid Schedule Type: {cfg.SOLVER.SCHEDULE.TYPE}")
