import torch
import torch.nn
from typing import Dict

from vgqa.utils.misc import to_device
from vgqa.utils.comm import synchronize, is_main_process
from tqdm import tqdm


@torch.no_grad()
def linear_interp(bbox_dict):
    """Perform linear interpolation between bounding boxes"""
    frame_ids = sorted([fid for fid in bbox_dict])
    if len(frame_ids) < 2:
        return bbox_dict
    for idx in range(0, len(frame_ids) - 1):
        left_fid = frame_ids[idx]
        right_fid = frame_ids[idx + 1]
        if right_fid - left_fid > 1:
            interval = right_fid - left_fid
            delta_x1 = (bbox_dict[right_fid][0][0] - bbox_dict[left_fid][0][0]) / interval
            delta_y1 = (bbox_dict[right_fid][0][1] - bbox_dict[left_fid][0][1]) / interval
            delta_x2 = (bbox_dict[right_fid][0][2] - bbox_dict[left_fid][0][2]) / interval
            delta_y2 = (bbox_dict[right_fid][0][3] - bbox_dict[left_fid][0][3]) / interval
            for step in range(1, interval):
                bbox_dict[left_fid + step] = [[
                  bbox_dict[left_fid][0][0] + step * delta_x1, 
                  bbox_dict[left_fid][0][1] + step * delta_y1, 
                  bbox_dict[left_fid][0][2] + step * delta_x2, 
                  bbox_dict[left_fid][0][3] + step * delta_y2, 
                ]]
    
    frame_ids = sorted([fid for fid in bbox_dict])
    assert max(frame_ids) - min(frame_ids) + 1 == len(frame_ids) 
    return {fid : bbox_dict[fid] for fid in frame_ids}


@torch.no_grad()
def linear_interp_conf(conf_dict):
    """Perform linear interpolation for confidence scores"""
    frame_ids = sorted([fid for fid in conf_dict])
    if len(frame_ids) < 2:
        return conf_dict
    for idx in range(0, len(frame_ids) - 1):
        left_fid = frame_ids[idx]
        right_fid = frame_ids[idx + 1]
        if right_fid - left_fid > 1:
            interval = right_fid - left_fid
            for step in range(1, interval):
                if step <= int(interval/2):
                    conf_dict[left_fid + step] = conf_dict[left_fid]
                else:
                    conf_dict[left_fid + step] = conf_dict[right_fid]

    frame_ids = sorted([fid for fid in conf_dict])
    assert max(frame_ids) - min(frame_ids) + 1 == len(frame_ids)
    return {fid: conf_dict[fid] for fid in frame_ids}

@torch.no_grad()
def single_forward(cfg, model, videos, texts, targets, device, postprocessor):
    """Run single forward pass for evaluation"""
    # Extract video durations
    durations = videos.durations
    targets[0]["durations"] = durations
    outputs = model(videos, texts, targets)

    # Prepare batch dimensions
    b = len(durations)
    t = max(durations)
    batch_img_size = [list(target['ori_size']) for target in targets]
    orig_target_sizes = [img_size for img_size in batch_img_size for _ in range(t)]
    orig_target_sizes = torch.tensor(orig_target_sizes,device=device)
    assert orig_target_sizes.shape[0] == outputs['pred_boxes'].shape[0]

    # Post-process predictions
    frames_ids = [target['frame_ids'] for target in targets]
    pred_boxs, pred_att, pred_steds, pred_kf = postprocessor(outputs, orig_target_sizes, frames_ids, durations)

    pred_boxs = pred_boxs.view(b, t, 4)

    # Organize predictions by video ID
    vids = [target['item_id'] for target in targets]
    bbox_pred, temp_pred, kf_pred = {}, {}, {}
    att_pred = {}

    # Collect bounding box and attention predictions
    for i_b in range(b):
        frames_id = frames_ids[i_b]
        bbox_pred[vids[i_b]] = {}
        att_pred[vids[i_b]] = {}
        assert durations[i_b] == len(frames_id)
        for idx in range(durations[i_b]):
            bbox_pred[vids[i_b]][frames_id[idx]] = [pred_boxs[i_b][idx].detach().cpu().tolist()]
            att_pred[vids[i_b]][frames_id[idx]] = [pred_att[i_b][idx].detach().cpu().tolist()]

    # Collect temporal predictions
    qtypes = [target['qtype'] for target in targets]
    assert len(pred_steds) == len(qtypes)
    for i_b in range(b):
        temp_pred[vids[i_b]] = {
            "sted": pred_steds[i_b],
            "qtype": qtypes[i_b],
        }
    kf_pred[vids[0]] = pred_kf
    return bbox_pred, att_pred, temp_pred, kf_pred
    

@torch.no_grad()
def do_eval(cfg, mode, logger, model, postprocessor, data_loader, evaluator, device):
    """Evaluate video spatial-temporal grounding model"""
    # Set model to evaluation mode
    model.eval()
    logger.info("Start evaluation on the {} split of {} dataset".format(mode, cfg.DATASET.NAME))

    for _, batch_dict in enumerate(tqdm(data_loader)):
        # Move data to device
        videos = batch_dict['videos'].to(device)
        texts = batch_dict['texts']
        targets = to_device(batch_dict["targets"], device)

        # Set default qtype if not present
        for i in range(len(targets)):
            if 'qtype' not in targets[i]:
                targets[i]['qtype'] = 'none'

        # Subsample videos at even frames
        videos1 = videos.subsample(2, start_idx=0)
        targets1 = [{'item_id': target['item_id'], 'ori_size': target['ori_size'], 'vid': target['vid'], 'orignal_frame': target['orignal_frame'][0::2],
                     'qtype': target['qtype'], 'frame_ids': target['frame_ids'][0::2], "boxs":target['boxs'].bbox.clone(), 'actioness':target['actioness'][0::2], "eval":True} for target in targets]

        # Subsample videos at odd frames
        videos2 = videos.subsample(2, start_idx=1)
        targets2 = [{'item_id': target['item_id'], 'ori_size': target['ori_size'], 'vid': target['vid'], 'orignal_frame': target['orignal_frame'][1::2],
                     'qtype': target['qtype'], 'frame_ids': target['frame_ids'][1::2], "boxs":target['boxs'].bbox.clone(), 'actioness':target['actioness'][1::2], "eval":True} for target in targets]

        # Align bounding boxes with action frames
        if torch.where(targets[0]["actioness"])[0][0] % 2 == 0:
            targets1[0]['boxs'] = targets1[0]['boxs'][0::2]
            targets2[0]['boxs'] = targets2[0]['boxs'][1::2]
        else:
            targets1[0]['boxs'] = targets1[0]['boxs'][1::2]
            targets2[0]['boxs'] = targets2[0]['boxs'][0::2]

        # Run forward pass on even frames
        bbox_pred1, att_pred1, temp_pred1, kf_pred1 = single_forward(cfg, model, videos1, texts,
                                targets1, device, postprocessor)
        # Run forward pass on odd frames
        bbox_pred2, att_pred2, temp_pred2, kf_pred2 = single_forward(cfg, model, videos2, texts,
                                targets2, device, postprocessor)

        # Merge and interpolate predictions
        bbox_pred, att_pred, temp_pred, kf_pred = {}, {}, {}, {}
        for vid in bbox_pred1:
            bbox_pred1[vid].update(bbox_pred2[vid])
            bbox_pred[vid] = linear_interp(bbox_pred1[vid])
            att_pred1[vid].update(att_pred2[vid])
            att_pred[vid] = linear_interp_conf(att_pred1[vid])
            kf_pred[vid] = [(kf_pred1[vid][0] + kf_pred2[vid][0])/2, (kf_pred1[vid][1] + kf_pred2[vid][1])/2]
            temp_pred[vid] = {'sted' : [min(temp_pred1[vid]['sted'][0], temp_pred2[vid]['sted'][0]),
                              max(temp_pred1[vid]['sted'][1], temp_pred2[vid]['sted'][1])]}
            if 'qtype' in temp_pred1[vid]:
                temp_pred[vid]['qtype'] = temp_pred1[vid]['qtype']

        # Update evaluator with predictions
        evaluator.update(bbox_pred)
        evaluator.update_att(att_pred)
        evaluator.update_kf_pr(kf_pred)
        evaluator.video_update(temp_pred)

    # Synchronize across processes
    synchronize()
    evaluator.synchronize_between_processes()
    if is_main_process():
        logger.info(f"Complete the inference on {mode} split of {cfg.DATASET.NAME}")

    # Compute and return evaluation metrics
    res = evaluator.summarize()
    return res